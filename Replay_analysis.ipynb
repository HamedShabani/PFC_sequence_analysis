{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook loads data from \"Get_clustered_seq.ipynb\" and performs replay analysis. the output of this script will be used by \"Replay_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustration of the use of these functions\n",
      " Mask indexes (burst array indexes): \n",
      " [2, 10, 50] \n",
      "Mask indexes expanded to its 2 neighbours: \n",
      " [ 0  1  2  3  4  8  9 10 11 12 48 49 50 51 52] \n",
      "Input indexes (array indexes of the positions y or spike positions ysp)\n",
      " [3, 7, 12, 30, 51, 100] \n",
      "Input indexes excluding those in the set of mask indexes\n",
      " [  7  30 100]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.special import factorial\n",
    "from scipy.stats import spearmanr\n",
    "import pftools as pf\n",
    "import configparser\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "# Initialize the configuration parser\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# # Read the configuration file\n",
    "# config.read('config.ini')\n",
    "\n",
    "# # Get the data folder path from the config file\n",
    "# savefolder= config['paths']['savefolder']\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['xtick.labelsize'] = 16  # You can adjust the font size as needed\n",
    "plt.rcParams['ytick.labelsize'] = 16  # You can adjust the font size as needed\n",
    "import mtools as mot\n",
    "import pickle\n",
    "from scipy.stats import mode\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Directory: c:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\n",
      "Skeletons Folder: C:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\\data\\GCaMP6f_478-485\\skeleton\\\n",
      "Save Folder: C:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\\results_new\\\n",
      "Data Folder: C:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\\data\\GCaMP6f_478-485\\\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import os\n",
    "from pathlib import Path\n",
    "onfig = configparser.ConfigParser()\n",
    "# Detect base directory\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent  # For scripts\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()  # For Jupyter Notebooks\n",
    "\n",
    "# Move one level up\n",
    "BASE_DIR = BASE_DIR.parent\n",
    "\n",
    "# Load config.ini\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.getcwd(), \"config.ini\"))\n",
    "\n",
    "# Construct full paths using pathlib and ensure they end with a separator\n",
    "skeletonsfolder = str((BASE_DIR / config[\"paths\"][\"skeletonsfolder\"]).resolve()) + os.sep\n",
    "savefolder = str((BASE_DIR / config[\"paths\"][\"savefolder\"]).resolve()) + os.sep\n",
    "datafolder = str((BASE_DIR / config[\"paths\"][\"datafolder\"]).resolve()) + os.sep\n",
    "\n",
    "# Print paths to verify\n",
    "print(\"Base Directory:\", BASE_DIR)\n",
    "print(\"Skeletons Folder:\", skeletonsfolder)\n",
    "print(\"Save Folder:\", savefolder)\n",
    "print(\"Data Folder:\", datafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'  # Export text as SVG <text> objects, not paths\n",
    "\n",
    "mpl.rcParams['xtick.major.size'] = 6    # Length of major ticks on x-axis\n",
    "mpl.rcParams['xtick.major.width'] = 1.5 # Width of major ticks on x-axis\n",
    "mpl.rcParams['xtick.direction'] = 'in' # Tick direction, both in and out\n",
    "mpl.rcParams['xtick.major.pad'] = 10    # Padding between ticks and x-axis labels\n",
    "\n",
    "mpl.rcParams['ytick.major.size'] = 6    # Length of major ticks on y-axis\n",
    "mpl.rcParams['ytick.major.width'] = 1.5 # Width of major ticks on y-axis\n",
    "mpl.rcParams['ytick.direction'] = 'in' # Tick direction, both in and out\n",
    "mpl.rcParams['ytick.major.pad'] = 10    # Padding between ticks and y-axis labels\n",
    "\n",
    "\n",
    "# Set default spine and tick settings globally for all plots\n",
    "mpl.rcParams['axes.linewidth'] = 1.5     # Set the width of all spines (top, bottom, left, right)\n",
    "mpl.rcParams['xtick.major.width'] = 1.5  # Width of major ticks on x-axis\n",
    "mpl.rcParams['ytick.major.width'] = 1.5  # Width of major ticks on y-axis\n",
    "mpl.rcParams['xtick.labelsize'] = 12     # Font size of x-axis tick labels\n",
    "mpl.rcParams['ytick.labelsize'] = 12     # Font size of y-axis tick labels\n",
    "\n",
    "# Define the helper function to hide spines\n",
    "def hide_spines(ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_allinfo(all_rates,P_x,r0,num_neurons):\n",
    "    all_I_f = np.zeros(num_neurons)  # Forward I\n",
    "\n",
    "    for nid in range(num_neurons):\n",
    "  \n",
    "        all_I_f[nid] = pf.computeSpatialInfo(all_rates[nid, :], P_x, r0[nid])\n",
    "    return(all_I_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase='learned'\n",
    "chosen_cell_type='all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this code i load the data from outward runs and find the cells which are place cells.\n",
    "#burst_len='200ms'\n",
    "fol='485'\n",
    "#dir_of_run='L'\n",
    "#celltype='PC'\n",
    "celltype='Alltest_neworder'\n",
    "fs = 20# sampling freq\n",
    "burst_length=0.5\n",
    "burst_len=str(burst_length)+'s'\n",
    "burstsize=burst_length*fs# sample numbers of a burst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = fol+'outward_'+dir_of_run+'_'+phase+'_correct_No_chunk_'+burst_len+'_transients_'+celltype+'_with_templates'\n",
    "filename = fol+'sampling_L sampling_R outward_L outward_R reward_L reward_R_'+phase+'_correct_No_chunk_'+burst_len+'_transients_'+celltype+'_with_templates'# load all at once reward outward sampling\n",
    "\n",
    "#filename_reward = fol+'reward_'+dir_of_run+'_'+phase+'_correct_No_chunk_'+burst_len+'_transients_'+celltype+'_with_templates'\n",
    "#filename_sampling = fol+'sampling_'+dir_of_run+'_'+phase+'_correct_No_chunk_'+burst_len+'_transients_'+celltype+'_with_templates'\n",
    "\n",
    "#filename = '478outward_R_all_correct_all_days_randomized_bursts_200ms_transients_allnew'\n",
    "\n",
    "data = np.load(join(savefolder, filename), allow_pickle=True)\n",
    "#data_reward = np.load(join(savefolder, filename_reward), allow_pickle=True)\n",
    "#data_sampling = np.load(join(savefolder, filename_sampling), allow_pickle=True)\n",
    "\n",
    "bound = [0, data['lin_pos'].shape[0]] #if turn_type == 'left' else [sepidx, endind]  \n",
    "\n",
    "target_cluids=np.unique(data['ids_clust'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['idpeaks_cells', 'mask_cond_fr_cells', 'ids_clust', 'idpeaks', 'seqs', 'bursts', 'mask_cond_fr', 'trial_idx_mask', 't', 'poprate', 'trace_cells', 'lin_pos', 'conditions', 'x_loc', 'y_loc', 'speed', 'fr', 'spike_idx', 'spike_idx_cells', 'sess_name', 'phase_name', 'cond_name', 'template'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['idpeaks_cells', 'mask_cond_fr_cells', 'ids_clust', 'idpeaks', 'seqs', 'bursts', 'mask_cond_fr', 'trial_idx_mask', 't', 'poprate', 'trace_cells', 'lin_pos', 'conditions', 'x_loc', 'y_loc', 'speed', 'fr', 'spike_idx', 'spike_idx_cells', 'sess_name', 'phase_name', 'cond_name', 'template'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, neuron_tspidx, burst_tidxs, burst_cluids, num_neurons, dt = pf.GetData(data, bound=bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds=['sampling_L',\n",
    " 'sampling_R',\n",
    " 'outward_L',\n",
    " 'outward_R',\n",
    " 'reward_L',\n",
    " 'reward_R',\n",
    " 'inward_L',\n",
    " 'inward_R']\n",
    "\n",
    "\n",
    "cond_number = dict(zip(conds, np.arange(len(conds))))# make mask values for each task\n",
    "cond_names=cond_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## to get the merged data of sampling, outward and reward for burst replay analysis \n",
    "# cell_types='All'\n",
    "# descriotor='No_chunk_0.5s_transients'\n",
    "# for fol in ['478','481','483','485']:\n",
    "\n",
    "#     #savefolder = 'C:/Users/hamed/Desktop/Hamed/Github/TP7/results/'\n",
    "#     #savefolder ='C:/Users/hamed/Desktop/Hamed/BCN/sequence/results/'\n",
    "#     #filename = '48520220215_gcamp6f485_task_learning.mat_all_sessions_all_days_transients'\n",
    "#     #filename = fol+'outward_'+dir_of_run+'_all_correct_all_days_original_bursts_neworder'+burst_len+'_transients_'+celltype\n",
    "#     #fol='481'\n",
    "#     #phase='all'\n",
    "#     filename = fol+'Sesseion_info_All AllNo_chunk_0.5s_transients'\n",
    "\n",
    "#     #filename = '478outward_R_all_correct_all_days_randomized_bursts_200ms_transients_allnew'\n",
    "\n",
    "#     data_all_tasks = np.load(join(savefolder, filename), allow_pickle=True)\n",
    "#     sess_info= data_all_tasks['sess_info']\n",
    "#     #Masks= data_all_tasks['Masks']# this mask does not include mask of correct trials of single cells\n",
    "\n",
    "\n",
    "#     filename_mask = fol+'Mask_with_correct_cells'\n",
    "#     Masks_ = np.load(join(savefolder, filename_mask), allow_pickle=True)# i added the correct masks for single cell data recently\n",
    "#     Masks=Masks_[0]\n",
    "\n",
    "\n",
    "#     #data_info=np.load(savefolder+fol+'data_all_sessions'+cell_types+'_'+ descriotor+'newodrer', allow_pickle=True)\n",
    "#     session_mask=Masks_[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     speed_thr=0\n",
    "#     # add the mask for speed. speed threshold is set to 5.\n",
    "#     smooth_speed=mot.smooth_signal(sess_info['speed'], 13)# moving average over .5  second\n",
    "#     Masks['speed_seq']=np.asarray(smooth_speed)[sess_info['id_peaks']]>speed_thr\n",
    "#     Masks['speed']=smooth_speed>speed_thr\n",
    "\n",
    "\n",
    "#     # select the data you need. Sessen number, epeoch, .... Selected data will be saved with corresposing name. Inward and outward data will be used by place filed code. \n",
    "#     sessin_numbers=np.arange(len(session_mask))#[4,5,6,7][0,1,2,3]##np.arange(len(session_mask))\n",
    "#     #sessin_numbers=[6]\n",
    "#     #sessin_numbers=[0,1,2,3,4,5,6,7] # soecify which session you want to work with\n",
    "#     #celid=23# number of sample cell to show in the plot\n",
    "\n",
    "#     #cond_number=[3] # conditon name (outwards)\n",
    "#     #cond_number=[8,9,10,11] # inwards\n",
    "#     #cond_number=[0,1] # sampling\n",
    "#     #cond_number=[6,7] # reward\n",
    "#     for con_number in ([1,3,5],[0,2,4],[0,2,4,1,3,5]):\n",
    "            \n",
    "#         trial_type=1 # 1 is correct       0 is failed\n",
    "\n",
    "#         if trial_type==1:\n",
    "#             type_name='correct'\n",
    "#         elif trial_type==0:\n",
    "#             type_name='failed'\n",
    "\n",
    "\n",
    "\n",
    "#         phase=None#None#None#,1 # 0 is learning,    1 is learned,  None is both\n",
    "#         if phase==None:\n",
    "#             phase_name='all'\n",
    "#         elif phase==0:\n",
    "#             phase_name='learning'\n",
    "#         elif phase==1:\n",
    "#             phase_name='learned'\n",
    "                \n",
    "\n",
    "\n",
    "#         # odd_even=1# even trials\n",
    "#         # run_data_e=mot.apply_masks(sess_info,Masks,cond_number,cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "#         # #plt.title(run_data_e['sess_name']+' (even) ('+ run_data_e['phase_name']+') ('+run_data_e['cond_name'] +')')\n",
    "\n",
    "#         # odd_even=0# odd trials\n",
    "#         # run_data_o=mot.apply_masks(sess_info,Masks,cond_number,cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "#         # #plt.title(run_data_e['sess_name']+' (odd) ('+ run_data_e['phase_name']+') ('+run_data_e['cond_name'] +')')\n",
    "\n",
    "#         odd_even=None# all trials\n",
    "#         run_data_all=mot.apply_masks_test(sess_info,Masks,con_number,cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "\n",
    "\n",
    "\n",
    "#         name_of_conds=' '.join([xn for xn,x in cond_names.items() if x in con_number])\n",
    "\n",
    "#         run_data_all['template']=sess_info['template']\n",
    "#         # with open(savefolder+fol+animal_direction+'_'+ phase_name+'_'+type_name+'_'+descriotor+'_'+cell_types+'test_neworder_with_templates','wb') as f:\n",
    "#         #     pickle.dump(dict(run_data_all), f)   \n",
    "#         #plt.title(run_data_e['sess_name']+' (all) ('+ run_data_e['phase_name']+') ('+run_data_e['cond_name'] +')')\n",
    "#         # with open(savefolder+fol+name_of_conds+'_'+ phase_name+'_'+type_name+'_'+descriotor+'_'+cell_types+'test_neworder_with_templates','wb') as f:\n",
    "#         #     pickle.dump(dict(run_data_all), f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis parameters\n",
    "exclude_peak_neighs=1\n",
    "include_peak_neighs=0\n",
    "\n",
    "\n",
    "dt = 1/fs  # bin size of each time index\n",
    "dy = 0.001 # bin size for position histogram. Since we will smooth the histogram anyways, the smaller the better (approaching Kernel Density Estimation).\n",
    "sigma_y = 0.05 # I do not know the size of the animal. So, assume it to be 1% of the track.\n",
    "sigma_yidx = sigma_y/dy  # Convert the position unit to unit of array index.\n",
    "yedges = np.arange(0-dy/2, 1+dy, dy)  # edges for histogram\n",
    "y_ax = pf.MidAx(yedges)  # position axis, mainly for plotting. Since edges are just for histogram.\n",
    "\n",
    "# Creating a mask array for excluding position bins at the burst events (with neighbouring number of index = 1)\n",
    "expanded_all_burst_tidxs = pf.expand_neighbins(burst_tidxs, expand_neighs=exclude_peak_neighs)\n",
    "\n",
    "y_tax = np.arange(y.shape[0])\n",
    "y_tax_excluded = pf.exclude_idx(y_tax, expanded_all_burst_tidxs)\n",
    "y_excluded = y[y_tax_excluded]\n",
    "\n",
    "\n",
    "# Compute the occupancy, using only those admitted position points\n",
    "occ, occ_gau = pf.GetOccupancy(y_excluded, yedges, dt, sigma_yidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rate_all(data,neuron_tspidx,occ_gau,yedges,sigma_yidx,expanded_all_burst_tidxs,num_neurons,y_ax,y,shuffle_spikes=False):\n",
    "    # This function uses the spike time to estimate the rate maps of all cells \n",
    "    # Compute the place fields\n",
    "    counts_all = np.zeros((num_neurons, y_ax.shape[0])).astype(float) # for storing spike counts. Not used.\n",
    "    p_all=  np.zeros(y_ax.shape[0]).astype(float)\n",
    "    r_all = np.zeros((num_neurons, y_ax.shape[0])).astype(float)  # for storing firing rates\n",
    "\n",
    "\n",
    "    for nid in range(num_neurons):\n",
    "        tspidx = neuron_tspidx[nid]\n",
    "\n",
    "        \n",
    "        # Exclude spike times occuring in the neighours(=1) of the burst events.\n",
    "        masked_tspidx = pf.exclude_idx(tspidx, expanded_all_burst_tidxs)\n",
    "        \n",
    "        if shuffle_spikes:# shuffle spike times for getting random place fields\n",
    "\n",
    "            masked_tspidx,spk=pf.spike_shuffle(data,masked_tspidx,None)\n",
    "\n",
    "\n",
    "        # Get \"spike positions\"\n",
    "        neuron_ysp = y[masked_tspidx]\n",
    "        \n",
    "        # Compute smoothed firing rate map\n",
    "        rate, counts_ysp_gau,P_x = pf.GetRate(neuron_ysp, occ_gau, yedges, sigma_yidx)\n",
    "\n",
    "        #print(len(masked_tspidx),len(masked_tspidx_sh))\n",
    "        \n",
    "        # Store all of them\n",
    "        r_all[nid, :] = rate\n",
    "        counts_all[nid, :] = counts_ysp_gau  \n",
    "        p_all=P_x\n",
    "    return r_all,counts_all,p_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all,counts_all,p_all=get_rate_all(data,neuron_tspidx,occ_gau,yedges,sigma_yidx,expanded_all_burst_tidxs,num_neurons,y_ax,y,shuffle_spikes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all_outward={}\n",
    "\n",
    "outward_l_single_cell=[]\n",
    "for inrn in range(len(neuron_tspidx)):\n",
    "\n",
    "    outward_l_single_cell.append(neuron_tspidx[inrn][(data['mask_cond_fr_cells'][inrn]==2)])\n",
    "\n",
    "outward_r_single_cell=[]\n",
    "for inrn in range(len(neuron_tspidx)):\n",
    "\n",
    "    outward_r_single_cell.append(neuron_tspidx[inrn][(data['mask_cond_fr_cells'][inrn]==3)])\n",
    "\n",
    "\n",
    "r_all_outward_left,counts_all_outward_left,p_all_outward_left= get_rate_all(data,outward_l_single_cell,occ_gau,yedges,sigma_yidx,expanded_all_burst_tidxs,num_neurons,y_ax,y)\n",
    "r_all_outward_right,counts_all_outward_right,p_all_outward_right= get_rate_all(data,outward_r_single_cell,occ_gau,yedges,sigma_yidx,expanded_all_burst_tidxs,num_neurons,y_ax,y)\n",
    "\n",
    "r_all_outward['left']=r_all_outward_left\n",
    "\n",
    "r_all_outward['right']=r_all_outward_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajtidx = np.arange(y.shape[0])\n",
    "# all_L_norm = np.zeros((y_ax.shape[0], trajtidx.shape[0]))\n",
    "\n",
    "\n",
    "# # Loop through all time indexes to decode the positions.\n",
    "# # Looping each time index is not so efficient. Vectorization is recommended if you do not need to include the neighbouring bins for decoding.\n",
    "# for trajtidx_each in trajtidx:\n",
    "#     #print('\\r%d/%d'%(trajtidx_each, trajtidx.shape[0]), end=\"\", flush=True)\n",
    "    \n",
    "#     # Decode each time index. \n",
    "#     _, L_norm = pf.decode_neighbins(trajtidx_each, y_ax, r_all, neuron_tspidx, dt=dt, num_neigh=include_peak_neighs)\n",
    "#     all_L_norm[:, trajtidx_each] = L_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_types=np.load( savefolder+'_Cell_types',allow_pickle=True)\n",
    "Cluster_types=np.load( savefolder+'_Cluster_types',allow_pickle=True)\n",
    "\n",
    "TC_clusters=np.concatenate((Cluster_types[phase][fol]['TC_arm'],Cluster_types[phase][fol]['TC_stem']))\n",
    "PC_clusters=np.concatenate((Cluster_types[phase][fol]['PC_arm'],Cluster_types[phase][fol]['PC_stem']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_type_idx={}\n",
    "mask_cell_type=np.zeros(len(neuron_tspidx),dtype=bool)# cells that are pc or tc are True\n",
    "\n",
    "mask_cell_type[Cell_types[phase][fol]['TC_arm']]=True\n",
    "mask_cell_type[Cell_types[phase][fol]['TC_stem']]=True\n",
    "mask_cell_type[Cell_types[phase][fol]['PC_arm']]=True\n",
    "mask_cell_type[Cell_types[phase][fol]['PC_stem']]=True\n",
    "\n",
    "Cell_type_idx['TC_cells']=np.union1d(Cell_types[phase][fol]['TC_arm'],Cell_types[phase][fol]['TC_stem'])\n",
    "Cell_type_idx['PC_cells']=np.union1d(Cell_types[phase][fol]['PC_arm'],Cell_types[phase][fol]['PC_stem'])\n",
    "\n",
    "# neuron_tspidx_TC=np.asarray([x for ix, x in enumerate(neuron_tspidx) if ix in TC_cells])\n",
    "# neuron_tspidx_PC=np.asarray([x for ix, x in enumerate(neuron_tspidx) if ix in PC_cells])\n",
    "\n",
    "\n",
    "Cell_type_idx['Non_SI_cells']=np.where(~mask_cell_type)[0]\n",
    "\n",
    "Cell_type_idx['All']=np.arange(len(neuron_tspidx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TC_cells\n",
      "PC_cells\n",
      "Non_SI_cells\n",
      "All\n"
     ]
    }
   ],
   "source": [
    "for cltyp_name,cltyp, in Cell_type_idx.items():\n",
    "    print(cltyp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster=np.unique(data['ids_clust'])\n",
    "\n",
    "all_cluster_tc_removed = np.setdiff1d(all_cluster, TC_clusters)\n",
    "SI_clusters = np.setdiff1d(all_cluster_tc_removed, PC_clusters)\n",
    "\n",
    "cluster_types={}\n",
    "cluster_types['TC_cluster']=TC_clusters\n",
    "cluster_types['PC_cluster']=PC_clusters\n",
    "cluster_types['Non_SI_cluster']=SI_clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_type_idx_all=Cell_type_idx['All']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Nshuffles = 500\n",
    "# Cell type and condition names\n",
    "#cell_types = ['TC_cells', 'PC_cells', 'Non_SI_cells','All']\n",
    "cond_names = {\n",
    "    'sampling_L': 0,\n",
    "    'sampling_R': 1,\n",
    "    'outward_L': 2,\n",
    "    'outward_R': 3,\n",
    "    'reward_L': 4,\n",
    "    'reward_R': 5,\n",
    "    'inward_L': 6,\n",
    "    'inward_R': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "def cound_spik_in_burst(burst_tidx,r_all,neuron_tspidx):\n",
    "    num_neigh=0\n",
    "    # Time bins, but for decoding\n",
    "    dt_total = dt * (num_neigh * 2 + 1)  # the larger the time bins, the larger the \"delta t\" for computing the rate.\n",
    "    tidx_min, tidx_max = burst_tidx - num_neigh, burst_tidx + num_neigh\n",
    "    \n",
    "    # Lengths/shapes\n",
    "    num_neurons = r_all.shape[0]\n",
    "    # spike counts:\n",
    "    counts_all = np.zeros(num_neurons)\n",
    "    for nid in range(num_neurons):\n",
    "        counts_all[nid] = np.sum((neuron_tspidx[nid] >= tidx_min) & (neuron_tspidx[nid] <= tidx_max)) # both sides inclusive.\n",
    "    return counts_all\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.ndimage import gaussian_filter1d\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from scipy.stats import pearsonr\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Input array (predicted animal positions)\n",
    "# positions =y_ML\n",
    "\n",
    "# # Step 1: Smooth the positions to reduce noise\n",
    "# smoothed_positions = positions#gaussian_filter1d(positions, sigma=1)\n",
    "\n",
    "# # Step 2: Calculate position differences\n",
    "# position_diff = np.diff(smoothed_positions)\n",
    "\n",
    "# # Step 3: Perform linear regression to fit a linear trajectory\n",
    "# time = np.arange(len(smoothed_positions)).reshape(-1, 1)\n",
    "# model = LinearRegression().fit(time, smoothed_positions)\n",
    "# slope = model.coef_[0]\n",
    "\n",
    "# # Step 4: Calculate correlation with a linear sequence\n",
    "# linear_sequence = np.linspace(min(smoothed_positions), max(smoothed_positions), len(smoothed_positions))\n",
    "# correlation, _ = pearsonr(smoothed_positions, linear_sequence)\n",
    "\n",
    "\n",
    "\n",
    "# # Step 6: Shuffle test for statistical significance\n",
    "# shuffled_correlations = []\n",
    "# for _ in range(1000):\n",
    "#     shuffled_positions = np.random.permutation(smoothed_positions)\n",
    "#     shuffled_corr, _ = pearsonr(shuffled_positions, linear_sequence)\n",
    "#     shuffled_correlations.append(shuffled_corr)\n",
    "\n",
    "# # Calculate p-value\n",
    "# p_value = np.mean(np.array(shuffled_correlations) >= correlation)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Slope: {slope:.3f}\")\n",
    "# print(f\"Correlation with linear sequence: {correlation:.3f}\")\n",
    "# print(f\"P-value from shuffle test: {p_value:.4f}\")\n",
    "# # Step 5: Plot the positions and linear fit\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(smoothed_positions, label=\"Smoothed Positions\", marker=\"o\")\n",
    "# plt.plot(linear_sequence, label=\"Linear Sequence\", linestyle=\"--\")\n",
    "# plt.title(f\"Slope: {slope:.3f}, Correlation: {correlation:.3f} , P: {p_value:.3f}\")\n",
    "# plt.legend()\n",
    "# plt.xlabel(\"Time\")\n",
    "# plt.ylabel(\"Position\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'  # Export text as SVG <text> objects, not paths\n",
    "\n",
    "mpl.rcParams['xtick.major.size'] = 6    # Length of major ticks on x-axis\n",
    "mpl.rcParams['xtick.major.width'] = 1.5 # Width of major ticks on x-axis\n",
    "mpl.rcParams['xtick.direction'] = 'in' # Tick direction, both in and out\n",
    "mpl.rcParams['xtick.major.pad'] = 10    # Padding between ticks and x-axis labels\n",
    "\n",
    "mpl.rcParams['ytick.major.size'] = 6    # Length of major ticks on y-axis\n",
    "mpl.rcParams['ytick.major.width'] = 1.5 # Width of major ticks on y-axis\n",
    "mpl.rcParams['ytick.direction'] = 'in' # Tick direction, both in and out\n",
    "mpl.rcParams['ytick.major.pad'] = 10    # Padding between ticks and y-axis labels\n",
    "\n",
    "\n",
    "# Set default spine and tick settings globally for all plots\n",
    "mpl.rcParams['axes.linewidth'] = 1.5     # Set the width of all spines (top, bottom, left, right)\n",
    "mpl.rcParams['xtick.major.width'] = 1.5  # Width of major ticks on x-axis\n",
    "mpl.rcParams['ytick.major.width'] = 1.5  # Width of major ticks on y-axis\n",
    "mpl.rcParams['xtick.labelsize'] = 12     # Font size of x-axis tick labels\n",
    "mpl.rcParams['ytick.labelsize'] = 12     # Font size of y-axis tick labels\n",
    "\n",
    "# Define the helper function to hide spines\n",
    "def hide_spines(ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the result dictionary for left and right separately\n",
    "# sig_ratios = {cell_type: {'left': {}, 'right': {}} for cell_type in cluster_types}\n",
    "# # List of condition groups to filter\n",
    "# #conditions_to_filter = ['reward',]\n",
    "# # Dictionary to map sides to outward conditions\n",
    "# cond_sides = {\n",
    "#     'left': cond_names['outward_L'],\n",
    "#     'right': cond_names['outward_R']\n",
    "# }\n",
    "# artifact_thr = .3\n",
    "# # Loop through each cell type\n",
    "# for cluster_type_name in cluster_types.keys():\n",
    "#     # Loop through left and right conditions\n",
    "#     for side, outward_cond in cond_sides.items():\n",
    "#         # Loop through all behavioral conditions (sampling, reward, outward)\n",
    "#         for condition,cond in cond_names.items():\n",
    "#             # Get the left or right condition for the current behavioral state\n",
    "#             cnd_name=condition\n",
    "#             #cond = cond_names[cnd_name]\n",
    "#             #print(str(outward_cond)+'       '+str(cond))\n",
    "#             # Create the burst condition mask using all conditions\n",
    "#             burst_cond_mask = (data['mask_cond_fr'] == cond)\n",
    "#             burst_idx_cond = burst_tidxs[burst_cond_mask]\n",
    "#             cluster_idx_cond = data['ids_clust'][burst_cond_mask]\n",
    "#             cluster_type_mask = np.isin(cluster_idx_cond, cluster_types[cluster_type_name])\n",
    "#             burst_cluster_type_cond = burst_idx_cond[cluster_type_mask]\n",
    "#             #print(condition)\n",
    "#             # Store in the dictionary\n",
    "#             #sig_ratios[cluster_type_name][side][condition] = burst_cluster_type_cond\n",
    "\n",
    "#             # Process outward_single_cell and r_all_type for each neuron using only outward condition\n",
    "#             outward_single_cell = []\n",
    "#             #r_all_type = []\n",
    "#             #for inrn in range(len(neuron_tspidx)):\n",
    "#                 # Only filter neurons based on the outward condition\n",
    "#                 #outward_single_cell.append(neuron_tspidx[inrn][data['mask_cond_fr_cells'][inrn] == outward_cond])\n",
    "#                # outward_single_cell.append(neuron_tspidx[inrn])\n",
    "\n",
    "#                 #r_all_type.append(r_all[inrn])\n",
    "\n",
    "#             # Optional: Store outward_single_cell and r_all_type if needed\n",
    "#             # Convert r_all_type to a NumPy array\n",
    "#             r_all_type = np.asarray(r_all_outward[side])\n",
    "\n",
    "#             # Space-time correlation parameters\n",
    "            \n",
    "#             include_peak_neighs2 = 0\n",
    "            \n",
    "#             pvals_1n = []\n",
    "#             pvals_1n_neg=[]\n",
    "#             z_bursts=[]\n",
    "#             Rmse=[]\n",
    "#             # Loop through all burst events\n",
    "#             for iburst_i, burst_i in enumerate(burst_cluster_type_cond):\n",
    "#                 burst_tidx = burst_i\n",
    "#                 frame_idxs = np.arange(burst_tidx - int(np.floor(burstsize / 2)), burst_tidx + int(np.ceil(burstsize / 2)))\n",
    "\n",
    "#                 all_L_norm = np.zeros((y_ax.shape[0], frame_idxs.shape[0]))\n",
    "#                 for i in range(len(frame_idxs)):\n",
    "#                     spk_cnt=cound_spik_in_burst(frame_idxs[i],r_all_type,neuron_tspidx)\n",
    "#                     if np.sum(spk_cnt)<1:\n",
    "#                     #print('boz')\n",
    "#                         continue\n",
    "                        \n",
    "#                     _, all_L_norm[:, i] = pf.decode_neighbins(frame_idxs[i], y_ax, r_all_type, neuron_tspidx, dt=dt, num_neigh=include_peak_neighs2)\n",
    "#                 non_zero_columns = np.where(~(all_L_norm == 0).all(axis=0))[0]\n",
    "\n",
    "#                 all_L_norm = all_L_norm[:, ~(all_L_norm == 0).all(axis=0)]\n",
    "#                 if len (all_L_norm)==0:\n",
    "#                     continue\n",
    "#                 #print('shape:',np.shape(all_L_norm))\n",
    "#                 y_real = y[frame_idxs[non_zero_columns]]\n",
    "\n",
    "#                 if 'outward' in condition : # if burst is on the edge of maze\n",
    "#                     # Identify the first point where the change exceeds the threshold\n",
    "\n",
    "#                     # Find the differences between consecutive elements\n",
    "#                     diff = np.abs(np.diff(y_real))\n",
    "#                     sudden_change_index = np.where(diff > artifact_thr)[0]\n",
    "\n",
    "#                     # Keep only points before the sudden change\n",
    "#                     if len(sudden_change_index) > 0:\n",
    "#                         y_real = y_real[:sudden_change_index[0] + 1]\n",
    "#                         all_L_norm = all_L_norm[:, :sudden_change_index[0] + 1]\n",
    "\n",
    "\n",
    "#                 if np.shape(all_L_norm)[1]<5:# exclude bursts hta have less than 5 active bins\n",
    "#                     continue\n",
    "\n",
    "#                 t_ax = np.arange(np.shape(all_L_norm)[1])\n",
    "                \n",
    "#                 yidx_max = np.nanargmax(all_L_norm, axis=0)\n",
    "\n",
    "#                 # if np.max(frame_idxs) >= len(y):\n",
    "#                 #     continue\n",
    "                \n",
    "\n",
    "                \n",
    "#                 # if abs(np.sum(np.diff(y_real))) > artifact_thr:\n",
    "#                 #     continue\n",
    "\n",
    "#                 # if np.sum(yidx_max == bias_idx_cell_types[side]) > 0:\n",
    "#                 #     continue\n",
    "\n",
    "#                 y_ML = y_ax[yidx_max]\n",
    "#                 # if np.sum(y_ML) == 0:\n",
    "#                 #     continue\n",
    "#                 rmse = np.sqrt(np.mean((y_real - y_ML) ** 2))\n",
    "#                 Rmse.append(rmse)\n",
    "#                 # Spearman correlation between predicted and real positions\n",
    "#                 rho, _ = spearmanr(y_ML, t_ax)\n",
    "#                 #rho, _ = pearsonr(y_ML, t_ax)\n",
    "#                 #print(rho)\n",
    "#                 # if np.isnan(rho):\n",
    "#                 #     continue\n",
    "                \n",
    "#                 # Generate a null distribution of correlations\n",
    "#                 null_rhos = np.zeros(Nshuffles)\n",
    "#                 for shui in range(Nshuffles):\n",
    "#                     ranvec = np.random.permutation(t_ax.shape[0])\n",
    "#                     null_rhos[shui], _ = spearmanr(y_ML, t_ax[ranvec])\n",
    "#                     #null_rhos[shui], _ = pearsonr(y_ML, t_ax[ranvec])\n",
    "\n",
    "#                 z= (rho-np.mean(null_rhos))/np.std(null_rhos)\n",
    "#                 z_bursts.append(z)\n",
    "#                 # Calculate p-value\n",
    "#                 pval = 1 - np.mean((rho) > (null_rhos))\n",
    "#                 pval_neg = 1 - np.mean((rho) < (null_rhos))# include negative replays\n",
    "\n",
    "#                 pvals_1n.append(pval)\n",
    "#                 pvals_1n_neg.append(pval_neg)\n",
    "#                 if 1:#((pval <.025) | (pval_neg < .025)) & ('outward' in condition):\n",
    "#                     # Plotting the heatmap and positions\n",
    "#                     fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "#                     # Heatmap of decoded positions\n",
    "#                     im = ax[0].pcolormesh(t_ax, y_ax, all_L_norm, cmap='hot', rasterized=True)\n",
    "#                     ax[0].scatter(t_ax, y_ML, c='#00008B', marker='x', s=50, label='ML')\n",
    "#                     ax[0].scatter(t_ax, y_real, c='g', marker='o', s=50, label='Real')\n",
    "#                     cbar = plt.colorbar(im, ax=ax[0])\n",
    "#                     cbar.ax.set_ylabel(r'P(spikes | position )')\n",
    "\n",
    "#                     ax[0].set_xlabel('Time index at burst', fontsize=16)\n",
    "#                     ax[0].set_ylabel('Position', fontsize=16)\n",
    "#                     ax[0].set_yticks(np.arange(0, 1, 0.1))\n",
    "#                     ax[0].set_yticks(np.arange(0, 1, 0.05), minor=True)\n",
    "#                     ax[0].set_ylim([-.05,1.05])\n",
    "#                     ax[0].legend(fontsize=12)\n",
    "\n",
    "#                     # Histogram of null distribution\n",
    "#                     ax[1].hist(null_rhos, bins=50, color='gray', alpha=0.7)\n",
    "#                     ax[1].axvline(rho, color='red', linestyle='--', label=f'Observed rho = {rho:.2f}')\n",
    "#                     ax[1].set_title(f'Null distribution  rmse={rmse:.5f} (p-val = {pval:.5f} or {pval_neg:.5f} )')\n",
    "#                     ax[1].legend()\n",
    "\n",
    "#                     plt.suptitle(f'{filename[:3]} {side} - {cluster_type_name} - {cnd_name}{phase}'+str(iburst_i), fontsize=18)\n",
    "#                     hide_spines(ax=ax[0])\n",
    "#                     fig.tight_layout()\n",
    "#                     plt.savefig(savefolder+'Replay'+side+f'{filename[:3]}{cluster_type_name}{cnd_name}{phase} burstid={iburst_i}.svg')\n",
    "#                     plt.show()\n",
    "\n",
    "#             # Calculate significant burst ratio\n",
    "#             #sig_ratio = np.sum(np.asarray(pvals_1n) < .05) / len(burst_cluster_type_cond)\n",
    "\n",
    "\n",
    "#             sig_ratio = np.sum((np.asarray(pvals_1n) < .025) | (np.asarray(pvals_1n_neg) < .025)) / len(burst_cluster_type_cond)\n",
    "\n",
    "            \n",
    "#             significant_bursts = {\n",
    "#                 'pvals_pos': pvals_1n,\n",
    "#                 'pvals_neg':pvals_1n_neg,\n",
    "#                 'sig_burst_ratio': sig_ratio,\n",
    "#                 'name': f'{filename[:3]} - {cluster_type_name} - {cnd_name}',\n",
    "#                 'Z':z_bursts,\n",
    "#                 'rmse':Rmse\n",
    "#             }\n",
    "\n",
    "#             # Store the result in the dictionary\n",
    "#             sig_ratios[cluster_type_name][side][condition] = significant_bursts\n",
    "#             significant_bursts_name = 'Replay' + filename[:3] + phase\n",
    "\n",
    "\n",
    "# # with open(savefolder+significant_bursts_name, 'wb') as file:\n",
    "# #     pickle.dump(sig_ratios, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamed\\AppData\\Local\\Temp\\ipykernel_11480\\3909277537.py:172: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  sig_ratio = np.sum((np.asarray(pvals_1n) < .025) | (np.asarray(pvals_1n_neg) < .025)) / len(burst_cluster_type_cond)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the result dictionary for left and right separately\n",
    "sig_ratios = {cell_type: {'left': {}, 'right': {}} for cell_type in cluster_types}\n",
    "# List of condition groups to filter\n",
    "#conditions_to_filter = ['reward',]\n",
    "# Dictionary to map sides to outward conditions\n",
    "cond_sides = {\n",
    "    'left': cond_names['outward_L'],\n",
    "    'right': cond_names['outward_R']\n",
    "}\n",
    "artifact_thr = .3\n",
    "# Loop through each cell type\n",
    "for cluster_type_name in cluster_types.keys():\n",
    "    # Loop through left and right conditions\n",
    "    for side, outward_cond in cond_sides.items():\n",
    "        # Loop through all behavioral conditions (sampling, reward, outward)\n",
    "        for condition,cond in cond_names.items():\n",
    "            # Get the left or right condition for the current behavioral state\n",
    "            cnd_name=condition\n",
    "            #cond = cond_names[cnd_name]\n",
    "            #print(str(outward_cond)+'       '+str(cond))\n",
    "            # Create the burst condition mask using all conditions\n",
    "            burst_cond_mask = (data['mask_cond_fr'] == cond)\n",
    "            burst_idx_cond = burst_tidxs[burst_cond_mask]\n",
    "            cluster_idx_cond = data['ids_clust'][burst_cond_mask]\n",
    "            cluster_type_mask = np.isin(cluster_idx_cond, cluster_types[cluster_type_name])\n",
    "            burst_cluster_type_cond = burst_idx_cond[cluster_type_mask]\n",
    "            #print(condition)\n",
    "            # Store in the dictionary\n",
    "            #sig_ratios[cluster_type_name][side][condition] = burst_cluster_type_cond\n",
    "\n",
    "            # Process outward_single_cell and r_all_type for each neuron using only outward condition\n",
    "            outward_single_cell = []\n",
    "            #r_all_type = []\n",
    "            #for inrn in range(len(neuron_tspidx)):\n",
    "                # Only filter neurons based on the outward condition\n",
    "                #outward_single_cell.append(neuron_tspidx[inrn][data['mask_cond_fr_cells'][inrn] == outward_cond])\n",
    "               # outward_single_cell.append(neuron_tspidx[inrn])\n",
    "\n",
    "                #r_all_type.append(r_all[inrn])\n",
    "\n",
    "            # Optional: Store outward_single_cell and r_all_type if needed\n",
    "            # Convert r_all_type to a NumPy array\n",
    "            r_all_type = np.asarray(r_all_outward[side])\n",
    "\n",
    "            # Space-time correlation parameters\n",
    "            \n",
    "            include_peak_neighs2 = 0\n",
    "            \n",
    "            pvals_1n = []\n",
    "            pvals_1n_neg=[]\n",
    "            z_bursts=[]\n",
    "            Rmse=[]\n",
    "            # Loop through all burst events\n",
    "            for iburst_i, burst_i in enumerate(burst_cluster_type_cond):\n",
    "                burst_tidx = burst_i\n",
    "                frame_idxs = np.arange(burst_tidx - int(np.floor(burstsize / 2)), burst_tidx + int(np.ceil(burstsize / 2)))\n",
    "\n",
    "                all_L_norm = np.zeros((y_ax.shape[0], frame_idxs.shape[0]))\n",
    "                for i in range(len(frame_idxs)):\n",
    "                    spk_cnt=cound_spik_in_burst(frame_idxs[i],r_all_type,neuron_tspidx)\n",
    "                    if np.sum(spk_cnt)<1:\n",
    "                    #print('boz')\n",
    "                        continue\n",
    "                        \n",
    "                    _, all_L_norm[:, i] = pf.decode_neighbins(frame_idxs[i], y_ax, r_all_type, neuron_tspidx, dt=dt, num_neigh=include_peak_neighs2)\n",
    "                non_zero_columns = np.where(~(all_L_norm == 0).all(axis=0))[0]\n",
    "                all_L_norm_org=all_L_norm\n",
    "                all_L_norm = all_L_norm[:, ~(all_L_norm == 0).all(axis=0)]\n",
    "\n",
    "                if len (all_L_norm)==0:\n",
    "                    continue\n",
    "                #print('shape:',np.shape(all_L_norm))\n",
    "                y_real = y[frame_idxs[non_zero_columns]]\n",
    "                y_real_org = y[frame_idxs]\n",
    "\n",
    "                if 'outward' in condition : # if burst is on the edge of maze\n",
    "                    # Identify the first point where the change exceeds the threshold\n",
    "\n",
    "                    # Find the differences between consecutive elements\n",
    "                    diff = np.abs(np.diff(y_real))\n",
    "                    sudden_change_index = np.where(diff > artifact_thr)[0]\n",
    "\n",
    "                    # Keep only points before the sudden change\n",
    "                    if len(sudden_change_index) > 0:\n",
    "                        y_real = y_real[:sudden_change_index[0] + 1]\n",
    "                        all_L_norm = all_L_norm[:, :sudden_change_index[0] + 1]\n",
    "\n",
    "\n",
    "                if np.shape(all_L_norm)[1]<5:# exclude bursts hta have less than 5 active bins\n",
    "                    continue\n",
    "\n",
    "                t_ax = np.arange(np.shape(all_L_norm)[1])\n",
    "                t_ax_org = np.arange(np.shape(all_L_norm_org)[1])\n",
    "\n",
    "                yidx_max = np.nanargmax(all_L_norm, axis=0)\n",
    "                yidx_max_org = np.nanargmax(all_L_norm_org, axis=0)\n",
    "\n",
    "                # if np.max(frame_idxs) >= len(y):\n",
    "                #     continue\n",
    "                \n",
    "\n",
    "                \n",
    "                # if abs(np.sum(np.diff(y_real))) > artifact_thr:\n",
    "                #     continue\n",
    "\n",
    "                # if np.sum(yidx_max == bias_idx_cell_types[side]) > 0:\n",
    "                #     continue\n",
    "\n",
    "                y_ML = y_ax[yidx_max]\n",
    "                y_ML_org = y_ax[yidx_max_org]\n",
    "\n",
    "                # if np.sum(y_ML) == 0:\n",
    "                #     continue\n",
    "                rmse = np.sqrt(np.mean((y_real - y_ML) ** 2))\n",
    "                Rmse.append(rmse)\n",
    "                # Spearman correlation between predicted and real positions\n",
    "                rho, _ = spearmanr(y_ML, t_ax)\n",
    "                #rho, _ = pearsonr(y_ML, t_ax)\n",
    "                #print(rho)\n",
    "                # if np.isnan(rho):\n",
    "                #     continue\n",
    "                \n",
    "                # Generate a null distribution of correlations\n",
    "                null_rhos = np.zeros(Nshuffles)\n",
    "                for shui in range(Nshuffles):\n",
    "                    ranvec = np.random.permutation(t_ax.shape[0])\n",
    "                    null_rhos[shui], _ = spearmanr(y_ML, t_ax[ranvec])\n",
    "                    #null_rhos[shui], _ = pearsonr(y_ML, t_ax[ranvec])\n",
    "\n",
    "                z= (rho-np.mean(null_rhos))/np.std(null_rhos)\n",
    "                z_bursts.append(z)\n",
    "                # Calculate p-value\n",
    "                pval = 1 - np.mean((rho) > (null_rhos))\n",
    "                pval_neg = 1 - np.mean((rho) < (null_rhos))# include negative replays\n",
    "\n",
    "                pvals_1n.append(pval)\n",
    "                pvals_1n_neg.append(pval_neg)\n",
    "                if 0:#pval <.05:#1:#((pval <.025) | (pval_neg < .025)) & ('outward' in condition):\n",
    "                    # Plotting the heatmap and positions\n",
    "                    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "                    # Heatmap of decoded positions\n",
    "                    im = ax[0].pcolormesh(t_ax_org, y_ax, all_L_norm_org, cmap='hot', rasterized=True)\n",
    "                    ax[0].scatter(t_ax_org, y_ML_org, c='#00008B', marker='x', s=50, label='ML')\n",
    "                    ax[0].scatter(t_ax_org, y_real_org, c='g', marker='o', s=50, label='Real')\n",
    "                    cbar = plt.colorbar(im, ax=ax[0])\n",
    "                    cbar.ax.set_ylabel(r'P(spikes | position )')\n",
    "\n",
    "                    ax[0].set_xlabel('Time index at burst', fontsize=16)\n",
    "                    ax[0].set_ylabel('Position', fontsize=16)\n",
    "                    ax[0].set_yticks(np.arange(0, 1, 0.1))\n",
    "                    ax[0].set_yticks(np.arange(0, 1, 0.05), minor=True)\n",
    "                    ax[0].set_ylim([-.05,1.05])\n",
    "                    ax[0].legend(fontsize=12)\n",
    "\n",
    "                    # Histogram of null distribution\n",
    "                    ax[1].hist(null_rhos, bins=50, color='gray', alpha=0.7)\n",
    "                    ax[1].axvline(rho, color='red', linestyle='--', label=f'Observed rho = {rho:.2f}')\n",
    "                    ax[1].set_title(f'Null distribution  rmse={rmse:.5f} (p-val = {pval:.5f} or {pval_neg:.5f} )')\n",
    "                    ax[1].legend()\n",
    "\n",
    "                    plt.suptitle(f'{filename[:3]} {side} - {cluster_type_name} - {cnd_name}{phase}'+str(iburst_i), fontsize=18)\n",
    "                    hide_spines(ax=ax[0])\n",
    "                    fig.tight_layout()\n",
    "                    plt.savefig(savefolder+'Replayfull'+side+f'{filename[:3]}{cluster_type_name}{cnd_name}{phase} burstid={iburst_i}.svg')\n",
    "                    plt.show()\n",
    "\n",
    "            # Calculate significant burst ratio\n",
    "            #sig_ratio = np.sum(np.asarray(pvals_1n) < .05) / len(burst_cluster_type_cond)\n",
    "\n",
    "\n",
    "            sig_ratio = np.sum((np.asarray(pvals_1n) < .025) | (np.asarray(pvals_1n_neg) < .025)) / len(burst_cluster_type_cond)\n",
    "\n",
    "            \n",
    "            significant_bursts = {\n",
    "                'pvals_pos': pvals_1n,\n",
    "                'pvals_neg':pvals_1n_neg,\n",
    "                'sig_burst_ratio': sig_ratio,\n",
    "                'name': f'{filename[:3]} - {cluster_type_name} - {cnd_name}',\n",
    "                'Z':z_bursts,\n",
    "                'rmse':Rmse\n",
    "            }\n",
    "\n",
    "            # Store the result in the dictionary\n",
    "            sig_ratios[cluster_type_name][side][condition] = significant_bursts\n",
    "            significant_bursts_name = 'Replay' + filename[:3] + phase\n",
    "\n",
    "\n",
    "with open(savefolder+significant_bursts_name, 'wb') as file:\n",
    "    pickle.dump(sig_ratios, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Replay485learned'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_bursts_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
