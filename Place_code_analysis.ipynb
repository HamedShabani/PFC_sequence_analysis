{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads the data of outward runs and finds the significant spatially informaive cells/clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.special import factorial\n",
    "from scipy.stats import spearmanr\n",
    "import pftools as pf\n",
    "import configparser\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "# Initialize the configuration parser\n",
    "# config = configparser.ConfigParser()\n",
    "\n",
    "# # Read the configuration file\n",
    "# config.read('config.ini')\n",
    "\n",
    "# # Get the data folder path from the config file\n",
    "# savefolder= config['paths']['savefolder']\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['xtick.labelsize'] = 16  # You can adjust the font size as needed\n",
    "plt.rcParams['ytick.labelsize'] = 16  # You can adjust the font size as needed\n",
    "import mtools as mot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Directory: c:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\n",
      "Skeletons Folder: C:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\\data\\GCaMP6f_478-485\\skeleton\\\n",
      "Save Folder: C:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\\results_new\\\n",
      "Data Folder: C:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\\data\\GCaMP6f_478-485\\\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Detect base directory\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent  # For scripts\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()  # For Jupyter Notebooks\n",
    "\n",
    "# Move one level up\n",
    "BASE_DIR = BASE_DIR.parent\n",
    "\n",
    "# Load config.ini\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join(os.getcwd(), \"config.ini\"))\n",
    "\n",
    "# Construct full paths using pathlib and ensure they end with a separator\n",
    "skeletonsfolder = str((BASE_DIR / config[\"paths\"][\"skeletonsfolder\"]).resolve()) + os.sep\n",
    "savefolder = str((BASE_DIR / config[\"paths\"][\"savefolder\"]).resolve()) + os.sep\n",
    "datafolder = str((BASE_DIR / config[\"paths\"][\"datafolder\"]).resolve()) + os.sep\n",
    "\n",
    "# Print paths to verify\n",
    "print(\"Base Directory:\", BASE_DIR)\n",
    "print(\"Skeletons Folder:\", skeletonsfolder)\n",
    "print(\"Save Folder:\", savefolder)\n",
    "print(\"Data Folder:\", datafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'  # Export text as SVG <text> objects, not paths\n",
    "\n",
    "mpl.rcParams['xtick.major.size'] = 6    # Length of major ticks on x-axis\n",
    "mpl.rcParams['xtick.major.width'] = 1.5 # Width of major ticks on x-axis\n",
    "mpl.rcParams['xtick.direction'] = 'in' # Tick direction, both in and out\n",
    "mpl.rcParams['xtick.major.pad'] = 10    # Padding between ticks and x-axis labels\n",
    "\n",
    "mpl.rcParams['ytick.major.size'] = 6    # Length of major ticks on y-axis\n",
    "mpl.rcParams['ytick.major.width'] = 1.5 # Width of major ticks on y-axis\n",
    "mpl.rcParams['ytick.direction'] = 'in' # Tick direction, both in and out\n",
    "mpl.rcParams['ytick.major.pad'] = 10    # Padding between ticks and y-axis labels\n",
    "\n",
    "\n",
    "# Set default spine and tick settings globally for all plots\n",
    "mpl.rcParams['axes.linewidth'] = 1.5     # Set the width of all spines (top, bottom, left, right)\n",
    "mpl.rcParams['xtick.major.width'] = 1.5  # Width of major ticks on x-axis\n",
    "mpl.rcParams['ytick.major.width'] = 1.5  # Width of major ticks on y-axis\n",
    "mpl.rcParams['xtick.labelsize'] = 12     # Font size of x-axis tick labels\n",
    "mpl.rcParams['ytick.labelsize'] = 12     # Font size of y-axis tick labels\n",
    "\n",
    "# Define the helper function to hide spines\n",
    "def hide_spines(ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_allinfo(all_rates,P_x,r0,num_neurons):\n",
    "    all_I_f = np.zeros(num_neurons)  # Forward I\n",
    "\n",
    "    for nid in range(num_neurons):\n",
    "  \n",
    "        all_I_f[nid] = pf.computeSpatialInfo(all_rates[nid, :], P_x, r0[nid])\n",
    "    return(all_I_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase='learning'\n",
    "chosen_cell_type='all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from outward runs and find place cells.\n",
    "#burst_len='200ms'\n",
    "fol='481'\n",
    "dir_of_run='L'\n",
    "#celltype='PC'\n",
    "celltype='Alltest_neworder'\n",
    "fs = 20# sampling freq\n",
    "burst_length=0.5\n",
    "burst_len=str(burst_length)+'s'\n",
    "burstsize=burst_length*fs# sample numbers of a burst\n",
    "\n",
    "shuffle_number=200# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hamed\\\\Desktop\\\\Hamed\\\\Github\\\\TP7\\\\results_new\\\\'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = fol+'outward_'+dir_of_run+'_'+phase+'_correct_No_chunk_'+burst_len+'_transients_'+celltype+'_with_templates'\n",
    "#filename = fol+'sampling_L outward_L reward_L_'+phase+'_correct_No_chunk_'+burst_len+'_transients_'+celltype+'_with_templates'# load all at once reward outward sampling\n",
    "\n",
    "filename = fol+'outward_'+dir_of_run+'_'+phase+'_correct'# Analyze place code with data of outward runs\n",
    "#filename_sampling = fol+'sampling_'+dir_of_run+'_'+phase+'_correct_No_chunk_'+burst_len+'_transients_'+celltype+'_with_templates'\n",
    "\n",
    "#filename = '478outward_R_all_correct_all_days_randomized_bursts_200ms_transients_allnew'\n",
    "\n",
    "data = np.load(join(savefolder, filename), allow_pickle=True)\n",
    "#data_reward = np.load(join(savefolder, filename_reward), allow_pickle=True)\n",
    "#data_sampling = np.load(join(savefolder, filename_sampling), allow_pickle=True)\n",
    "\n",
    "bound = [0, data['lin_pos'].shape[0]] #if turn_type == 'left' else [sepidx, endind]  \n",
    "\n",
    "target_cluids=np.unique(data['ids_clust'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='485_arena_arena'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdfg\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfg' is not defined"
     ]
    }
   ],
   "source": [
    "dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, neuron_tspidx, burst_tidxs, burst_cluids, num_neurons, dt = pf.GetData(data, bound=bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mboz\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boz' is not defined"
     ]
    }
   ],
   "source": [
    "boz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#burst_tidxs_reward=data_reward['spike_idx'][0]\n",
    "#burst_tidxs_sampling=data_sampling['spike_idx'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/hamed/Desktop/Hamed/Github/TP7/results_new/'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds=['sampling_L',\n",
    " 'sampling_R',\n",
    " 'outward_L',\n",
    " 'outward_R',\n",
    " 'reward_L',\n",
    " 'reward_R',\n",
    " 'inward_L',\n",
    " 'inward_R']\n",
    "\n",
    "\n",
    "cond_number = dict(zip(conds, np.arange(len(conds))))# make mask values for each task\n",
    "cond_names=cond_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_data={'idpeaks_cells':[[] for _ in range(len(sess_info['Spike_times_cells']))]}\n",
    "# run_data['idpeaks']=np.asarray(sess_info['id_peaks'])\n",
    "# t_all=np.asarray(sess_info['t'])\n",
    "# spk_times=np.where(np.isin(t_all,run_data['idpeaks']))# time of spike for population rate in the determined condtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cell_types='All'\n",
    "# descriotor='No_chunk_0.5s_transients'\n",
    "# for fol in ['478','481','483','485']:\n",
    "\n",
    "#     #savefolder = 'C:/Users/hamed/Desktop/Hamed/Github/TP7/results/'\n",
    "#     #savefolder ='C:/Users/hamed/Desktop/Hamed/BCN/sequence/results/'\n",
    "#     #filename = '48520220215_gcamp6f485_task_learning.mat_all_sessions_all_days_transients'\n",
    "#     #filename = fol+'outward_'+dir_of_run+'_all_correct_all_days_original_bursts_neworder'+burst_len+'_transients_'+celltype\n",
    "#     #fol='481'\n",
    "#     #phase='all'\n",
    "#     filename = fol+'Sesseion_info_All AllNo_chunk_0.5s_transients'\n",
    "\n",
    "#     #filename = '478outward_R_all_correct_all_days_randomized_bursts_200ms_transients_allnew'\n",
    "\n",
    "#     data_all_tasks = np.load(join(savefolder, filename), allow_pickle=True)\n",
    "#     sess_info= data_all_tasks['sess_info']\n",
    "#     Masks= data_all_tasks['Masks']\n",
    "\n",
    "#     data_info=np.load(savefolder+fol+'data_all_sessions'+cell_types+'_'+ descriotor+'newodrer', allow_pickle=True)\n",
    "#     session_mask=data_info[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     speed_thr=0\n",
    "#     # add the mask for speed. speed threshold is set to 5.\n",
    "#     smooth_speed=mot.smooth_signal(sess_info['speed'], 13)# moving average over .5  second\n",
    "#     Masks['speed_seq']=np.asarray(smooth_speed)[sess_info['id_peaks']]>speed_thr\n",
    "#     Masks['speed']=smooth_speed>speed_thr\n",
    "\n",
    "\n",
    "#     # select the data you need. Sessen number, epeoch, .... Selected data will be saved with corresposing name. Inward and outward data will be used by place filed code. \n",
    "#     sessin_numbers=np.arange(len(session_mask))#[4,5,6,7][0,1,2,3]##np.arange(len(session_mask))\n",
    "#     #sessin_numbers=[6]\n",
    "#     #sessin_numbers=[0,1,2,3,4,5,6,7] # soecify which session you want to work with\n",
    "#     #celid=23# number of sample cell to show in the plot\n",
    "\n",
    "#     #cond_number=[3] # conditon name (outwards)\n",
    "#     #cond_number=[8,9,10,11] # inwards\n",
    "#     #cond_number=[0,1] # sampling\n",
    "#     #cond_number=[6,7] # reward\n",
    "#     for con_number in ([0,2,4,]):\n",
    "            \n",
    "#         trial_type=1 # 1 is correct       0 is failed\n",
    "\n",
    "#         if trial_type==1:\n",
    "#             type_name='correct'\n",
    "#         elif trial_type==0:\n",
    "#             type_name='failed'\n",
    "\n",
    "\n",
    "\n",
    "#         phase=1#None#None#,1 # 0 is learning,    1 is learned,  None is both\n",
    "#         if phase==None:\n",
    "#             phase_name='all'\n",
    "#         elif phase==0:\n",
    "#             phase_name='learning'\n",
    "#         elif phase==1:\n",
    "#             phase_name='learned'\n",
    "                \n",
    "\n",
    "\n",
    "#         # odd_even=1# even trials\n",
    "#         # run_data_e=mot.apply_masks(sess_info,Masks,cond_number,cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "#         # #plt.title(run_data_e['sess_name']+' (even) ('+ run_data_e['phase_name']+') ('+run_data_e['cond_name'] +')')\n",
    "\n",
    "#         # odd_even=0# odd trials\n",
    "#         # run_data_o=mot.apply_masks(sess_info,Masks,cond_number,cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "#         # #plt.title(run_data_e['sess_name']+' (odd) ('+ run_data_e['phase_name']+') ('+run_data_e['cond_name'] +')')\n",
    "\n",
    "#         odd_even=None# all trials\n",
    "#         run_data_all=mot.apply_masks_test(sess_info,Masks,[con_number],cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "\n",
    "\n",
    "\n",
    "#         reversed_mapping = {v: k for k, v in cond_names.items()}\n",
    "#         print(reversed_mapping.get(con_number,'Key not found'),len(run_data_all['trace_cells']))\n",
    "#         animal_direction=reversed_mapping.get(con_number,'Key not found')\n",
    "#         #animal_direction = mot.find_condition(cond_number,cond_names)\n",
    "#         #print(animal_direction)\n",
    "#         run_data_all['template']=sess_info['template']\n",
    "#         # with open(savefolder+fol+animal_direction+'_'+ phase_name+'_'+type_name+'_'+descriotor+'_'+cell_types+'test_neworder_with_templates','wb') as f:\n",
    "#         #     pickle.dump(dict(run_data_all), f)   \n",
    "#         #plt.title(run_data_e['sess_name']+' (all) ('+ run_data_e['phase_name']+') ('+run_data_e['cond_name'] +')')\n",
    "#         with open(savefolder+fol+animal_direction+'_'+ phase_name+'_'+type_name+'_'+descriotor+'_'+cell_types+'test_neworder_with_templates','wb') as f:\n",
    "#             pickle.dump(dict(run_data_all), f) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'481'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cell_types='All'\n",
    "# descriotor='No_chunk_0.5s_transients'\n",
    "# for fol in ['478','481','483','485']:\n",
    "\n",
    "#     savefolder = 'C:/Users/hamed/Desktop/Hamed/Github/TP7/results/'\n",
    "#     #savefolder ='C:/Users/hamed/Desktop/Hamed/BCN/sequence/results/'\n",
    "#     #filename = '48520220215_gcamp6f485_task_learning.mat_all_sessions_all_days_transients'\n",
    "#     #filename = fol+'outward_'+dir_of_run+'_all_correct_all_days_original_bursts_neworder'+burst_len+'_transients_'+celltype\n",
    "#     #fol='481'\n",
    "#     #phase='all'\n",
    "#     filename = fol+'Sesseion_info_All AllNo_chunk_0.5s_transients'\n",
    "\n",
    "#     #filename = '478outward_R_all_correct_all_days_randomized_bursts_200ms_transients_allnew'\n",
    "\n",
    "#     data_all_tasks = np.load(join(savefolder, filename), allow_pickle=True)\n",
    "#     sess_info= data_all_tasks['sess_info']\n",
    "#     #Masks= data_all_tasks['Masks']# this mask does not include mask of correct trials of single cells\n",
    "\n",
    "\n",
    "#     filename_mask = fol+'Mask_with_correct_cells'\n",
    "#     Masks_ = np.load(join(savefolder, filename_mask), allow_pickle=True)# i added the correct masks for single cell data recently\n",
    "#     Masks=Masks_[0]\n",
    "\n",
    "\n",
    "#     #data_info=np.load(savefolder+fol+'data_all_sessions'+cell_types+'_'+ descriotor+'newodrer', allow_pickle=True)\n",
    "#     session_mask=Masks_[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     speed_thr=0\n",
    "#     # add the mask for speed. speed threshold is set to 5.\n",
    "#     smooth_speed=mot.smooth_signal(sess_info['speed'], 13)# moving average over .5  second\n",
    "#     Masks['speed_seq']=np.asarray(smooth_speed)[sess_info['id_peaks']]>speed_thr\n",
    "#     Masks['speed']=smooth_speed>speed_thr\n",
    "\n",
    "\n",
    "#     # select the data you need. Sessen number, epeoch, .... Selected data will be saved with corresposing name. Inward and outward data will be used by place filed code. \n",
    "#     sessin_numbers=np.arange(len(session_mask))#[4,5,6,7][0,1,2,3]##np.arange(len(session_mask))\n",
    "#     #sessin_numbers=[6]\n",
    "#     #sessin_numbers=[0,1,2,3,4,5,6,7] # soecify which session you want to work with\n",
    "#     #celid=23# number of sample cell to show in the plot\n",
    "\n",
    "#     #cond_number=[3] # conditon name (outwards)\n",
    "#     #cond_number=[8,9,10,11] # inwards\n",
    "#     #cond_number=[0,1] # sampling\n",
    "#     #cond_number=[6,7] # reward\n",
    "#     for con_number in ([0,2,4],[1,3,5]):\n",
    "            \n",
    "#         trial_type=1 # 1 is correct       0 is failed\n",
    "\n",
    "#         if trial_type==1:\n",
    "#             type_name='correct'\n",
    "#         elif trial_type==0:\n",
    "#             type_name='failed'\n",
    "\n",
    "\n",
    "\n",
    "#         phase=0#None#None#,1 # 0 is learning,    1 is learned,  None is both\n",
    "#         if phase==None:\n",
    "#             phase_name='all'\n",
    "#         elif phase==0:\n",
    "#             phase_name='learning'\n",
    "#         elif phase==1:\n",
    "#             phase_name='learned'\n",
    "                \n",
    "\n",
    "\n",
    "#         # odd_even=1# even trials\n",
    "#         # run_data_e=mot.apply_masks(sess_info,Masks,cond_number,cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "#         # #plt.title(run_data_e['sess_name']+' (even) ('+ run_data_e['phase_name']+') ('+run_data_e['cond_name'] +')')\n",
    "\n",
    "#         # odd_even=0# odd trials\n",
    "#         # run_data_o=mot.apply_masks(sess_info,Masks,cond_number,cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "#         # #plt.title(run_data_e['sess_name']+' (odd) ('+ run_data_e['phase_name']+') ('+run_data_e['cond_name'] +')')\n",
    "\n",
    "#         odd_even=None# all trials\n",
    "#         run_data_all=mot.apply_masks_test(sess_info,Masks,con_number,cond_names,sessin_numbers,odd_even,session_mask,trial_type,phase)\n",
    "\n",
    "\n",
    "\n",
    "#         name_of_conds=' '.join([xn for xn,x in cond_names.items() if x in con_number])\n",
    "\n",
    "#         run_data_all['template']=sess_info['template']\n",
    "\n",
    "#         # with open(savefolder+fol+name_of_conds+'_'+ phase_name+'_'+type_name+'_'+descriotor+'_'+cell_types+'test_neworder_with_templates','wb') as f:\n",
    "#         #     pickle.dump(dict(run_data_all), f) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_data_out=run_data\n",
    "# t_all_out_out=t_all_out\n",
    "# mask_cond_t_out=mask_cond_t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_data_sam=run_data\n",
    "# t_all_out_sam=t_all_out\n",
    "# mask_cond_t_sam=mask_cond_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis parameters\n",
    "exclude_peak_neighs=1\n",
    "include_peak_neighs=0\n",
    "\n",
    "\n",
    "dt = 1/fs  # bin size of each time index\n",
    "dy = 0.001 # bin size for position histogram. Since we will smooth the histogram anyways, the smaller the better (approaching Kernel Density Estimation).\n",
    "sigma_y = 0.05 # I do not know the size of the animal. So, assume it to be 1% of the track.\n",
    "sigma_yidx = sigma_y/dy  # Convert the position unit to unit of array index.\n",
    "yedges = np.arange(0-dy/2, 1+dy, dy)  # edges for histogram\n",
    "y_ax = pf.MidAx(yedges)  # position axis, mainly for plotting. Since edges are just for histogram.\n",
    "\n",
    "# Creating a mask array for excluding position bins at the burst events (with neighbouring number of index = 1)\n",
    "expanded_all_burst_tidxs = pf.expand_neighbins(burst_tidxs, expand_neighs=exclude_peak_neighs)\n",
    "\n",
    "y_tax = np.arange(y.shape[0])\n",
    "y_tax_excluded = pf.exclude_idx(y_tax, expanded_all_burst_tidxs)\n",
    "y_excluded = y[y_tax_excluded]\n",
    "\n",
    "\n",
    "# Compute the occupancy, using only those admitted position points\n",
    "occ, occ_gau = pf.GetOccupancy(y_excluded, yedges, dt, sigma_yidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/hamed/Desktop/Hamed/Github/TP7/results_new/'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occ_gau=occ_gau+1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1261"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\\PFC_sequence_analysis\\pftools.py:114: RuntimeWarning: invalid value encountered in divide\n",
      "  rate = counts_ysp_gau / occ_gau\n",
      "c:\\Users\\hamed\\Desktop\\Hamed\\Github\\TP7\\PFC_sequence_analysis\\pftools.py:115: RuntimeWarning: invalid value encountered in divide\n",
      "  P_x_f = occ_gau/occ_gau.sum()\n"
     ]
    }
   ],
   "source": [
    "r_all,counts_all,p_all= pf.get_rate_all(data,neuron_tspidx,occ_gau,yedges,sigma_yidx,expanded_all_burst_tidxs,num_neurons,y,y_ax,shuffle_spikes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r_shuffled_cells_all=[]\n",
    "pvals=np.zeros(num_neurons)\n",
    "\n",
    "for sh in range(shuffle_number):\n",
    "\n",
    "    p_all_sh=np.zeros_like(p_all)\n",
    "    counts_all_sh=np.zeros_like(counts_all)\n",
    "    r_all_sh=np.zeros_like(r_all)\n",
    "\n",
    "    spikes=[]\n",
    "    spikes_sh=[]\n",
    "\n",
    "    r_all_sh,counts_all_sh,p_all_sh= pf.get_rate_all(data,neuron_tspidx,occ_gau,yedges,sigma_yidx,expanded_all_burst_tidxs,num_neurons,y,y_ax,shuffle_spikes=True)\n",
    "\n",
    "    r_shuffled_cells_all.append(r_all_sh)\n",
    "\n",
    "\n",
    "    r0 = pf.mean_rate2(r_all, p_all,num_neurons)\n",
    "    all_I=compute_allinfo(r_all,p_all,r0[0],num_neurons)\n",
    "\n",
    "    r0_sh = pf.mean_rate2(r_all_sh, p_all_sh,num_neurons)\n",
    "    all_I_sh=compute_allinfo(r_all_sh,p_all_sh,r0_sh[0],num_neurons)\n",
    "\n",
    "\n",
    "    pvals+=all_I>all_I_sh\n",
    "\n",
    "    \n",
    "pf_pvals=1-pvals/shuffle_number    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajtidx = np.arange(y.shape[0])\n",
    "all_L_norm = np.zeros((y_ax.shape[0], trajtidx.shape[0]))\n",
    "\n",
    "\n",
    "# Loop through all time indexes to decode the positions.\n",
    "# Looping each time index is not so efficient. Vectorization is recommended if you do not need to include the neighbouring bins for decoding.\n",
    "for trajtidx_each in trajtidx:\n",
    "    print('\\r%d/%d'%(trajtidx_each, trajtidx.shape[0]), end=\"\", flush=True)\n",
    "    \n",
    "    # Decode each time index. \n",
    "    _, L_norm = pf.decode_neighbins(trajtidx_each, y_ax, r_all, neuron_tspidx, dt=dt, num_neigh=include_peak_neighs)\n",
    "    all_L_norm[:, trajtidx_each] = L_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign=pf_pvals<.05\n",
    "insign=pf_pvals>=.05\n",
    "\n",
    "place_cell_data={}\n",
    "place_cell_data['rate_all']=r_all\n",
    "place_cell_data['rate_all_shuffled']=r_shuffled_cells_all\n",
    "\n",
    "\n",
    "place_cell_data['significant']=sign\n",
    "place_cell_data['pvalues']=pf_pvals\n",
    "place_cell_data['insignificant']=insign\n",
    "\n",
    "place_cell_data['all_L_norm']=all_L_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hamed\\\\Desktop\\\\Hamed\\\\Github\\\\TP7\\\\results_new\\\\'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m biased_y \u001b[38;5;241m=\u001b[39m mode(y_ML)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#print('The biased y estimator value is ', biased_y, '\\nPlease make sure the value corresponds to the most overrepresented ML estimator in the trajectory decoding above.')\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m biased_yidx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_ax \u001b[38;5;241m==\u001b[39m \u001b[43mbiased_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "# Take the maximum of posterior probabilities across the position as the prediction \n",
    "# It is the maximum likelihood (ML) estimator.\n",
    "yidx_max = np.nanargmax(all_L_norm, axis=0)\n",
    "y_ML = y_ax[yidx_max]\n",
    "# Identify the position bin that is the bias, to be excluded\n",
    "\n",
    "biased_y = mode(y_ML)\n",
    "#print('The biased y estimator value is ', biased_y, '\\nPlease make sure the value corresponds to the most overrepresented ML estimator in the trajectory decoding above.')\n",
    "biased_yidx = np.where(y_ax == biased_y.mode[0])[0][0]\n",
    "#print('The index of the position bin corresponding to this biased value \\n =', biased_yidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only get the cluster that has at least 3 burst\n",
    "thr_burst_number=4\n",
    "target_cluids_long=[]\n",
    "for cid,target_cluid in enumerate(target_cluids):\n",
    "    \n",
    "    # Select the burst events that belong to this cluster id\n",
    "    clu_idxs = np.where(burst_cluids == target_cluid)[0]\n",
    "    num_clu_bursts = len(clu_idxs)\n",
    "    burst_tidx = burst_tidxs[clu_idxs]\n",
    "    #print(len(burst_tidx))\n",
    "    if len(burst_tidx)>thr_burst_number:\n",
    "        target_cluids_long.append(target_cluid)\n",
    "target_cluids_long= np.array(target_cluids_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_all_burst_tidxs = pf.expand_neighbins(burst_tidxs, expand_neighs=exclude_peak_neighs)\n",
    "expanded_all_burst_tidxs=[]\n",
    "num_clusters=len(target_cluids_long)\n",
    "target_cluids_long\n",
    "r_all_cluster,counts_all_cluster,p_all_cluster= pf.get_rate_all_clusters(data,target_cluids_long,y,occ_gau,yedges,sigma_yidx,expanded_all_burst_tidxs,num_neurons,y_ax,burst_tidxs,burst_cluids,shuffle_spikes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute place field of clusters\n",
    "\n",
    "num_clusters=len(target_cluids_long)\n",
    "pvals_cluster=np.zeros(num_clusters)\n",
    "rate_shuffled_all=[]\n",
    "for sh in range(shuffle_number):\n",
    "\n",
    "\n",
    "    p_all_sh=np.zeros_like(p_all)\n",
    "    counts_all_sh=np.zeros_like(counts_all)\n",
    "    r_all_sh=np.zeros_like(r_all)\n",
    "\n",
    "    spikes=[]\n",
    "    spikes_sh=[]\n",
    "\n",
    "\n",
    "    expanded_all_burst_tidxs = pf.expand_neighbins(burst_tidxs, expand_neighs=exclude_peak_neighs)\n",
    "    expanded_all_burst_tidxs=[]\n",
    "    #r_all_cluster = np.zeros((num_clusters, y_ax.shape[0])).astype(float)  # for storing firing rates\n",
    "\n",
    "    r_all_cluster_sh,counts_all_cluster_sh,p_all_cluster_sh= pf.get_rate_all_clusters(data,target_cluids_long,y,occ_gau,yedges,sigma_yidx,expanded_all_burst_tidxs,num_neurons,y_ax,burst_tidxs,burst_cluids,shuffle_spikes=True)\n",
    "\n",
    "\n",
    "    rate_shuffled_all.append(r_all_cluster_sh)\n",
    "\n",
    "    r0 = pf.mean_rate2(r_all_cluster, p_all_cluster,num_clusters)\n",
    "    all_I=compute_allinfo(r_all_cluster,p_all_cluster,r0[0],num_clusters)\n",
    "\n",
    "\n",
    "    r0_sh = pf.mean_rate2(r_all_cluster_sh, p_all_cluster_sh,num_clusters)\n",
    "    all_I_sh=compute_allinfo(r_all_cluster_sh,p_all_cluster_sh,r0_sh[0],num_clusters)\n",
    "\n",
    "\n",
    "    pvals_cluster+= all_I>all_I_sh\n",
    "pf_pvals_clusters=1-pvals_cluster/shuffle_number    \n",
    "# Plot place fields as a heatmap\n",
    "# fig, (ax,ax2) = plt.subplots(1, 2,figsize=(8, 4))\n",
    "\n",
    "# im = ax.pcolormesh(y_ax, np.arange(num_clusters), r_all_cluster)\n",
    "# ax.set_yticks(np.arange(num_clusters))\n",
    "# ax.set_yticklabels(target_cluids_long)\n",
    "\n",
    "# plt.colorbar(im, ax=ax)\n",
    "# plt.title('original ')\n",
    "\n",
    "# plt.subplot(122)\n",
    "# #fig, ax = plt.subplots(figsize=(16, 8))\n",
    "# im = ax2.pcolormesh(y_ax, np.arange(num_clusters), r_all_cluster_sh)\n",
    "# ax2.set_yticks(np.arange(num_clusters))\n",
    "# ax2.set_yticklabels(target_cluids_long)\n",
    "\n",
    "# plt.colorbar(im, ax=ax2)\n",
    "# plt.title('shuffled ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamed\\AppData\\Local\\Temp\\ipykernel_2628\\2371685144.py:7: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  significant_Clusters['significant_ratio']=np.sum(sign)/len(sign)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sign=pf_pvals_clusters<.05\n",
    "insign=pf_pvals_clusters>=.05\n",
    "significant_Clusters={}\n",
    "significant_Clusters['singnificnts_cluster_ids']=target_cluids_long[sign]\n",
    "significant_Clusters['insingnificnts_cluster_ids']=target_cluids_long[insign]\n",
    "significant_Clusters['significant_ratio']=np.sum(sign)/len(sign)\n",
    "significant_Clusters['pvalues']=pf_pvals_clusters\n",
    "significant_Clusters['cluster_numbers']=target_cluids_long\n",
    "significant_Clusters['rate_significant']=r_all_cluster[sign]\n",
    "significant_Clusters['rate_insignificant']=r_all_cluster[insign]\n",
    "significant_Clusters['rate_all_clusters']=r_all_cluster\n",
    "significant_Clusters['rate_shuffled_clusters']=rate_shuffled_all\n",
    "#significant_Clusters['replay_insignificant']=significant_bursts\n",
    "significant_Clusters['burst_tidxs']=burst_tidxs\n",
    "\n",
    "\n",
    "significant_cluster_name = savefolder+filename+'_Cluster'\n",
    "\n",
    "\n",
    "\n",
    "# # Save the significant_Clusters\n",
    "with open(significant_cluster_name, 'wb') as file:\n",
    "    pickle.dump(significant_Clusters, file)\n",
    "\n",
    "# Save the rate maps of cells and clusters\n",
    "place_cell_data['Clusters']=significant_Clusters\n",
    "place_cell_data_name = savefolder+filename+'_Cell'\n",
    "\n",
    "# # Save the boolean vector to a file\n",
    "with open(place_cell_data_name, 'wb') as file:\n",
    "    pickle.dump(place_cell_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [139]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mboz\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boz' is not defined"
     ]
    }
   ],
   "source": [
    "boz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fol1='483'#fol#'478'\n",
    "phases=['learning','learned']\n",
    "direction=['L','R']\n",
    "PC_phase_data={}\n",
    "PCclstr_phase_data={}\n",
    "\n",
    "sessinfo_and_mask=np.load(savefolder+fol1+'Sesseion_info_All AllNo_chunk_0.5s_transients',allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "for phase in phases:\n",
    "    PC_dir_data={}\n",
    "    PClstr_dir_data={}\n",
    "\n",
    "    for dir in direction:\n",
    "        #cluster_pc_fractions=np.load(savefolder+fol1+'outward_'+dir+'_'+phase+'_correct_No_chunk_0.5s_transients_Alltest_neworder_with_templates_place_cell_data'+chosen_cell_type,allow_pickle=True)\n",
    "        PC_dir_data[dir]=np.load(savefolder+fol1+'outward_'+dir+'_'+phase+'_correct_Cell',allow_pickle=True)\n",
    "        PClstr_dir_data[dir]=np.load(savefolder+fol1+'outward_'+dir+'_'+phase+'_correct_Cluster',allow_pickle=True)\n",
    "\n",
    "    PC_phase_data[phase]=PC_dir_data\n",
    "    PCclstr_phase_data[phase]=PClstr_dir_data\n",
    "\n",
    "\n",
    "sessinfo_and_mask['place_cells_data']=PC_phase_data\n",
    "sessinfo_and_mask['place_cluster_data']=PCclstr_phase_data\n",
    "\n",
    "\n",
    "\n",
    "with open(savefolder+fol1+'Sesseion_info'+'_with_PC','wb') as f:\n",
    "    pickle.dump(sessinfo_and_mask, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_Clusters['cluster_numbers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the place code of bursts in each cluster id (membership)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['xtick.labelsize'] = 18  # You can adjust the font size as needed\n",
    "plt.rcParams['ytick.labelsize'] = 18 # You can adjust the font size as needed\n",
    "\n",
    "# Loop through all the cluster ids\n",
    "for target_cluid in significant_Clusters['cluster_numbers']:# only shows clusters with minimum 5 events in them\n",
    "    \n",
    "    # Select the burst events that belong to this cluster id\n",
    "    clu_idxs = np.where(burst_cluids == target_cluid)[0]\n",
    "    num_clu_bursts = len(clu_idxs)\n",
    "    \n",
    "    # Decode each burst event with its 2 neighbouring bins\n",
    "    num_neigh =1\n",
    "    all_L_norm = np.zeros((y_ax.shape[0], num_clu_bursts))\n",
    "    \n",
    "    # loop through all burst events that belong to this cluster id\n",
    "    # and decode the posterior prob\n",
    "    for i in range(num_clu_bursts):\n",
    "        clu_idx = clu_idxs[i]\n",
    "        burst_tidx = burst_tidxs[clu_idx]\n",
    "        _, L_norm = pf.decode_neighbins(burst_tidx, y_ax, r_all, neuron_tspidx, dt=0.05, num_neigh=num_neigh)\n",
    "        all_L_norm[:, i] = L_norm\n",
    "\n",
    "    # Take the maximum as the prediction\n",
    "    yidx_max = np.argmax(all_L_norm, axis=0)\n",
    "    y_ML = y_ax[yidx_max]\n",
    "\n",
    "    # Obtain the real positions\n",
    "    bursttidx_clu = burst_tidxs[clu_idxs]\n",
    "    y_real = y[bursttidx_clu]\n",
    "    \n",
    "    # Sort the real positions from low to high.\n",
    "    sort_idx = np.argsort(y_real)\n",
    "            \n",
    "    # Burst id axis, for plotting purpose.\n",
    "    burst_id_ax = np.arange(num_clu_bursts)\n",
    "    \n",
    "    \n",
    "    # Plot the posterior heatmap\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4), )\n",
    "    im = ax.pcolormesh(burst_id_ax, y_ax, all_L_norm[:, sort_idx])\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Plot the real and ML positions. Using the sorting index comptued before.\n",
    "    ax.scatter(burst_id_ax, y_ML[sort_idx], color='r', marker='x', s=50, label='ML')  # ML estimators\n",
    "    ax.scatter(burst_id_ax, y_real[sort_idx], color='b', marker='x', s=50, label='Real')  # Real positions\n",
    "    \n",
    "    ax.set_xlabel(\"Burst event id sorted by real position\",fontsize=18)\n",
    "    ax.set_ylabel('Position',fontsize=18)\n",
    "    #ax.legend(loc='lower right')\n",
    "    if target_cluid in significant_Clusters['singnificnts_cluster_ids']:\n",
    "        ax.set_title(' Decoded posterior for SI cluster %d '%(target_cluid)+' '+burst_len)\n",
    "    else:\n",
    "        ax.set_title(' Decoded posterior for Non-SI cluster %d '%(target_cluid)+' '+burst_len)\n",
    "    \n",
    "    # Plot the marginal sum of posterior probability \n",
    "    # separately for bursts occurring when the animal is at the middle stem or the arm.\n",
    "    # ax[1].plot(y_ax, all_L_norm[:, (y_real>=0) & (y_real < 0.5)].sum(axis=1), label='Mid')\n",
    "    # ax[1].plot(y_ax, all_L_norm[:, (y_real>=0.5)].sum(axis=1), label='Arm')\n",
    "\n",
    "    # #ax[1].scatter(all_L_norm[:, (y_real>=0) & (y_real < 0.5)].sum(axis=1), all_L_norm[:, (y_real>=0.5)].sum(axis=1), label='Arm vs mid')\n",
    "\n",
    "    # ax[1].set_xlabel('Position')\n",
    "    # ax[1].set_title('Marginal sum of posterior across all bursts, when the real position is in MidStem or Arm')\n",
    "    # ax[1].legend()\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    #fig.savefig(join(savefolder, '%s_ClusterPlaceCode-Clu-.png'%(target_cluid)), dpi=300, facecolor='w')\n",
    "    \n",
    "    fig.savefig(savefolder+'_ClusterPlaceCode-Clu'+str(target_cluid)+filename+burst_len+celltype+burst_len+'.png', facecolor='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
